{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FIT5196 Assessment 2\n",
    "# FIT5196 Task  in Assessment 2\n",
    "#### Student Name: Pattranit Chaiyabud and Viet Tai Le\n",
    "#### Student ID: 30304148 and 29975336 \n",
    "\n",
    "Date: 14/09/2019\n",
    "\n",
    "Version: 1.0\n",
    "\n",
    "Environment: Python 3.6.0 and Anaconda 4.3.0 (64-bit)\n",
    "\n",
    "Libraries used:\n",
    "* re (for regular expression, included in Anaconda Python 3.6.0)\n",
    "* os (for executing in subshell, included in Anaconda Python 3.6.0)\n",
    "* urllib (for for retrive the pdf from url, include in Anaconda Python 3.6.0)\n",
    "* itertools ( for iteration, include in Anaconda Python 3.6.0 )\n",
    "* nltk 3.2.2 (Natural Language Toolkit, included in Anaconda Python 3.6)\n",
    "* nltk.collocations (for finding bigrams, included in Anaconda Python 3.6)\n",
    "* nltk.tokenize (for tokenization, included in Anaconda Python 3.6)\n",
    "* nltk.probability (for frequency, included in Anaconda Python 3.6)\n",
    "* nltk.stem (for PorterStemmer, included in Anaconda Python 3.6)\n",
    "* nltk.corpus (for stopword, included in Anaconda Python 3.6)\n",
    "* pandas (for dataframe, included in Anaconda Python 3.6.0) \n",
    "\n",
    "\n",
    "\n",
    "## 1. Introduction\n",
    "This assignment found on the execution of different analyzing textual data such as extracting data from non-structured format and converting into the proper format suitable for a downstream modelling task. A data-set containing 200 URLs for paper published in a popular AI conference is given in the file named `Group148.pdf`. The required tasks are the following:\n",
    "\n",
    "Extracting 200 URLs from the given pdf-file then use the extracted URLs to programmatically download the PDF files and read into text. After that extract the required entities to complete the task below:\n",
    "1. Generate a sparse representation for `Paper Bodies` consisting of two of the following files\n",
    "    - Vocabulary index file\n",
    "    - Sparse count vectors file\n",
    "2. Generate a CSV file containing `top most frequent word` for three of the following columns\n",
    "    - Titles\n",
    "    - Authors\n",
    "    - Abstracts\n",
    "    \n",
    "More details for each task will be given in the following sections.\n",
    "## 2.  Import libraries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import nltk\n",
    "from nltk.collocations import *\n",
    "from itertools import chain\n",
    "import itertools\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.tokenize import MWETokenizer\n",
    "from nltk.tokenize import sent_tokenize \n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.probability import *\n",
    "import urllib\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Examining File  and  prepare Data\n",
    "As a first step, we need to use `pdfminer package` to convert pdf file `Group148.pdf` to txt file `Group148.txt`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\mio\\Anaconda3\\Scripts\\pdf2txt.py\", line 136, in <module>\n",
      "    if __name__ == '__main__': sys.exit(main())\n",
      "  File \"C:\\Users\\mio\\Anaconda3\\Scripts\\pdf2txt.py\", line 131, in main\n",
      "    outfp = extract_text(**vars(A))\n",
      "  File \"C:\\Users\\mio\\Anaconda3\\Scripts\\pdf2txt.py\", line 62, in extract_text\n",
      "    with open(fname, \"rb\") as fp:\n",
      "FileNotFoundError: [Errno 2] No such file or directory: 'Group148.pdf'\n"
     ]
    }
   ],
   "source": [
    "# convert pdf file to txt file\n",
    "!pdf2txt.py -o Group148.txt Group148.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let see how all URLs in text file look like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is how all URLs look like\n",
    "pdf_txt = open('./Group148.txt', 'r')\n",
    "# loop over all the lines\n",
    "for line in pdf_txt:\n",
    "    # repr() is a built-in Python fuction that returns a string containing a printable representation of an object.\n",
    "    print (repr(line))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above code read the text file line-by-line and printed each line. You should notice that we have converted each line into a printable representation of a string object using Python's build-in function, `repr()`, as it will help us discover some patterns that can be used to extract those URLs.\n",
    "\n",
    "Next we will use `re package` to extract the URLs from the pattern above and put them all together in a dictionary before further process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#open file and read line by line\n",
    "pdfTxtFile = './Group148.txt'\n",
    "with open(pdfTxtFile, 'r') as f:\n",
    "    pdf_txt=f.readlines()   \n",
    "# Create a dictionary contains pdf name and pdf links \n",
    "list_pdf=[]\n",
    "list_name=[]\n",
    "#spot the pattern of both name and URLs\n",
    "reg = re.compile(r'(https:.*)')\n",
    "regpdf=re.compile(r'(PP.*?)\\.')\n",
    "#extract and add URLs to the list\n",
    "for line in pdf_txt:\n",
    "    if reg.search(line):\n",
    "        list_pdf.append((reg.search(line).group(1)))\n",
    "#extract and add name to the list\n",
    "for line in pdf_txt:\n",
    "    if regpdf.search(line):\n",
    "        list_name.append((regpdf.search(line).group(1)))\n",
    "#combine both name and URLs together as key-value dictionary\n",
    "dict1=dict(zip(list_name,list_pdf))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After arranging the file in dictionary, we programmatically download the PDF files from the given URLs using `urlib.request` then read the PDF files into text. Now we are ready for all the task! \n",
    "\n",
    "Note that this step may a bit of time to execute because we have 200 files to perform. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download all pdf files using urllib.request\n",
    "for key,value in dict1.items():\n",
    "    urllib.request.urlretrieve (value, key+ \".pdf\")\n",
    "#Convert pdf files to txt files\n",
    "for key in dict1.keys():\n",
    "    os.system(\"pdf2txt.py -o %s %s\" % (key+'.txt', key+'.pdf'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Task 1\n",
    "\n",
    "Before building the sparse representation, you will need to perform text preprocessing on Paper Bodies. Please follow the below text preprocessing steps (not necessarily in the same order, you will need to figure out the correct order of operations that produces the correct set of vocabulary) \n",
    "\n",
    "A. The word tokenization must use the following regular expression, r\"[A-Za-z]\\w+(?:['?]\\w+)?\" \n",
    "\n",
    "B. The context-independent and context-dependent (with the threshold set to %95) stop words must be removed from the vocab. The context-independent stop words list (i.e, stopwords_en.txt) provided in the zip file must be used.\n",
    "\n",
    "C. Unigram tokens should be stemmed using the Porter stemmer. (be careful that stemming performs lower casing by default)\n",
    "\n",
    "D. Rare tokens (with the threshold set to 3%) must be removed from the vocab. \n",
    "\n",
    "E. Tokens must be normalized to lowercase except the capital tokens appearing in the middle of a sentence/line. (use sentence segmentation to achieve this) \n",
    "\n",
    "F. Tokens with the length less than 3 should be removed from the vocab.  \n",
    "\n",
    "G. First 200 meaningful bigrams  (i.e., collocations), based on highest total frequency in the corpus, must be extracted and included in your tokenization process. Bigrams should not include context-independent stopwords as part of them and they should be separated using double underscore i.e. “__”  (example: “artifical____intelligence”) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Firstly, we need to extract content of Paper body section by using re.search with patern `r'Paper Body(.*?)2 References'`. After that, we create a dictionary that contains document name and its paper body correspondently. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 1\n",
    "# Preparing data\n",
    "rawdict={}\n",
    "filetxt=[key+'.txt' for key in dict1.keys()]\n",
    "for file in filetxt:\n",
    "    rawdoc=''\n",
    "    with open(file,'r',encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            rawdoc += line.strip()\n",
    "        rawdata=re.search(r'Paper Body(.*?)2 References',rawdoc).group(1)\n",
    "        rawdict[file[:-4]]=rawdata\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>In order to implement task 1</b>, the order of text preprocessing steps is E,A,B,D,F,C. At first, i combined step E and A by define a function name <b>tokenize</b> which will process sentence Segmentation and lowercase the first word of each segment. After that, the function will tokenize all segment in each document. \n",
    "\n",
    "In the next step, after using for loop to tokenize all documents in corpus, the method chain.from_iterable can help to concat all document's tokens to a list of tokens for a whole corpus \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sentence segmentation,tokenizer\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "def doc_tokenize(doc):\n",
    "    segdoc=sent_tokenize(rawdict[doc])\n",
    "    tokenizer = RegexpTokenizer(r\"[A-Za-z]\\w+(?:['?]\\w+)?\")\n",
    "    tokens=[]\n",
    "    for sentence in segdoc:\n",
    "        list1=sentence.strip().split(' ')\n",
    "        list1[0]=list1[0].lower()\n",
    "        sentence=' '.join(list1)\n",
    "        tokens+=tokenizer.tokenize(sentence)\n",
    "    return (doc,tokens)\n",
    "\n",
    "doc_tokenized = dict(doc_tokenize(doc) for doc in rawdict.keys())\n",
    "all_words = list(chain.from_iterable(doc_tokenized.values()))\n",
    "doc_tokenized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next stage is creating top 200 bigram based on highest frequency in the corpus. In order to do that, we need *BigramAssocMeasures() \n",
    "*BigramCollocationFinder.from_words()\n",
    "*bigram_finder.apply_word_filter(lambda w: w in stopindewords)\n",
    "*nbest(bigram_measures.raw_freq, 200)\n",
    "\n",
    "To introduce top 200 bigrams to tokens, a method MWETokenizer which takes a string which has already been divided into tokens and retokenizes it, merging multi-word expressions into single tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create stopindewords\n",
    "with open('stopwords_en.txt','r') as f:\n",
    "    stopindewords=f.read().splitlines()\n",
    "# bigram\n",
    "bigram_measures = nltk.collocations.BigramAssocMeasures()\n",
    "bigram_finder = nltk.collocations.BigramCollocationFinder.from_words(all_words)\n",
    "bigram_finder.apply_word_filter(lambda w: w in stopindewords)\n",
    "# raw_freq to find top 200 based on high frequency \n",
    "top_200_bigrams = bigram_finder.nbest(bigram_measures.raw_freq, 200) \n",
    "# alternatively, can use ngram_fd.items() to find top 200\n",
    "# put bigram to tokens\n",
    "mwetokenizer = MWETokenizer(top_200_bigrams,separator='__')\n",
    "colloc_doc =  dict((key, mwetokenizer.tokenize(value)) for key,value in doc_tokenized.items())\n",
    "all_words_colloc = list(chain.from_iterable(colloc_doc.values()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step here is create sets that contains context dependent stop words and rare tokens. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create list of stopword\n",
    "words_1 = list(chain.from_iterable([set(value) for value in colloc_doc.values()]))\n",
    "fd_1 = FreqDist(words_1)\n",
    "# Threshold of rare tokens\n",
    "min_doc=0.03*len(rawdict)\n",
    "\n",
    "max_doc=0.95*len(rawdict)\n",
    "# set of rare tokens\n",
    "rare_tokens={token for token,value in fd_1.items() if value <= min_doc}\n",
    "# set of context dependent stopwords\n",
    "stopdewords={token for token,value in fd_1.items() if value >= max_doc }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using set comprehension and list comprehension to remove all stopwords,rare tokens and token with length less than 3 from vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab=set(all_words_colloc)\n",
    "# remove stopwords from vocab\n",
    "token_filtered={token for token in vocab if token not in (stopindewords and stopdewords )}\n",
    "# remove rare tokens from vocab\n",
    "removed_rare_token=[token for token in token_filtered if token not in rare_tokens]\n",
    "#remove token less 3 from vocab\n",
    "removed3_tokens=[token for token in removed_rare_token if len(token)>=3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since by default stem method lower case all words, we have to re-define method without lower casing words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import Porter Stemmer and change default parameter of stem \n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "def stem(self, word):\n",
    "        stem = word\n",
    "\n",
    "        if self.mode == self.NLTK_EXTENSIONS and word in self.pool:\n",
    "            return self.pool[word]\n",
    "\n",
    "        if self.mode != self.ORIGINAL_ALGORITHM and len(word) <= 2:\n",
    "            # With this line, strings of length 1 or 2 don't go through\n",
    "            # the stemming process, although no mention is made of this\n",
    "            # in the published algorithm.\n",
    "            return word\n",
    "\n",
    "        stem = self._step1a(stem)\n",
    "        stem = self._step1b(stem)\n",
    "        stem = self._step1c(stem)\n",
    "        stem = self._step2(stem)\n",
    "        stem = self._step3(stem)\n",
    "        stem = self._step4(stem)\n",
    "        stem = self._step5a(stem)\n",
    "        stem = self._step5b(stem)\n",
    "\n",
    "        return stem\n",
    "PorterStemmer.stem = stem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using stem method that we re-define above to stemming all unigram and after that remove all duplicate items in vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stemming token\n",
    "stemmer = PorterStemmer()\n",
    "vocab=[stemmer.stem(w) if '__' not in w else w for w in removed3_tokens]\n",
    "# remove all duplicate words from vocab\n",
    "vocab=sorted(set(vocab))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Export vocabulary into required format by creating a dictionary which keys are token and values are their index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export vocab to txt file\n",
    "vocab_output=''\n",
    "vocab_dict={token[1]:token[0] for token in enumerate(vocab)}\n",
    "for key,value in vocab_dict.items():\n",
    "    vocab_output+=key +':'+ str(value) +' \\n'\n",
    "with open('Group148_vocab.txt','w',encoding='utf-8') as f:\n",
    "    f.write(vocab_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To export count vector,firstly, we need to stemming all unigrams in each document's tokens. Secondly, we have to count term frequency of each words in a document by method FreqDist. After that, If else statement can help to filter token that in vocabulary and have frequency greater than 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create count vector and export it to txt file\n",
    "count_vector_corpus=''\n",
    "\n",
    "for doc in colloc_doc.keys():\n",
    "    # dictionary contains number of occurance of stemmed token in each document\n",
    "    # stemming unigram token in each doc\n",
    "    stemm_token=[]\n",
    "    stemm_token=[stemmer.stem(token) if '__' not in token else token for token in colloc_doc[doc]]\n",
    "    # counting orrcurance\n",
    "    fd_token=FreqDist(stemm_token)\n",
    "    # apply index of token in vocab to the above dictionary \n",
    "    count_vector={}\n",
    "    for token,w_count in fd_token.items():\n",
    "        if token in vocab_dict.keys() and w_count >0:\n",
    "            count_vector[vocab_dict[token]]=w_count\n",
    "    count_vector_corpus+=doc+','+str(count_vector).strip('{}').replace(' ','')+'\\n'\n",
    "with open('Group148_count_vectors.txt','w',encoding='utf-8') as f:\n",
    "    f.write(count_vector_corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Task 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before Statistics Generation for the second task we will need to perform the following preprocessing steps on the Titles and Abstracts before extracting the required stats:\n",
    "\n",
    "A. The word tokenization must use the following regular expression, `r\"[A-Za-z]\\w+(?:[-'?]\\w+)?\"`\n",
    "\n",
    "B. The context-independent stop words (i.e, stopwords_en.txt) must be removed\n",
    "\n",
    "C. For Abstracts, Tokens must be normalized to lowercase except the capital tokensappearing in the middle of a sentence/line. (use sentence segmentation to achieve this). For Titles, tokens must be all normalised to lowercase."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Starting with preparing the data before extracting by make the data in Dictionary(key,value). While key is the name of the file and value is the whole content of the file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Task2\n",
    "#preparing data\n",
    "files ={}\n",
    "filenames = [key+'.txt' for key in dict1.keys()]\n",
    "for each in filenames:\n",
    "    with open(each,'r', encoding=\"utf8\") as f:\n",
    "        if each in files:\n",
    "            continue\n",
    "        files[each] = f.read().strip()          "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next create 5 of the following functions to support each job of extraction and statistic generation.\n",
    "\n",
    "Create the first function called `clean()` to remove all the newline using `str.strip()` and `re.sub` to get cleaner look of data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean(value):\n",
    "    clean_list = [x.strip() for x in value]\n",
    "    clean_list = ''.join(clean_list)\n",
    "    clean_list = re.sub(r'\\n',' ',clean_list)\n",
    "    clean_list = re.sub(r'\\-\\s','',clean_list)\n",
    "    return clean_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The second function created is `get_author()`. This function join each line after extracting the author name in the next few step and differentiate the author name by `\\n` between names using `str.split()` then remove `''` from the list to get the whole list of authors' name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_author(value):\n",
    "    author_list = ''.join(value)\n",
    "    author_list = author_list.splitlines()\n",
    "    author_list = [x for x in author_list if x] \n",
    "    return author_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next two function is `get_title()` and `get_abstract()` function. After getting cleaned extracted data, we split the sentence into token by `[A-Za-z]\\w+(?:[-?]\\w+)?` for both of them and remove stop word at the end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_title(value):\n",
    "    title_list = clean(value)\n",
    "    tokenizer = RegexpTokenizer(r'[A-Za-z]\\w+(?:[-?]\\w+)?')\n",
    "    title_list = tokenizer.tokenize(title_list)\n",
    "    title_list = [x.lower() for x in title_list]\n",
    "    title_list = [x for x in title_list if x not in stopindewords]\n",
    "    return title_list    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before tokenization of `get_abstract()` function, we split sentence by `sent_tokenize()` since there might be capital word appearing in the middle of sentence/line which should not be lower cased as it would make them lose their meaning. Therefore, lowercase only the first letter of each sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_abstract(value):\n",
    "    abstract_list = clean(value)\n",
    "    abstract_list = sent_tokenize(abstract_list)\n",
    "    abstract_list = [x[0].lower() + x[1:] for x in abstract_list]\n",
    "    abstract_list = ''.join(abstract_list)\n",
    "    tokenizer = RegexpTokenizer(r'[A-Za-z]\\w+(?:[-?]\\w+)?')\n",
    "    abstract_list = tokenizer.tokenize(abstract_list)  \n",
    "    abstract_list = [x for x in abstract_list if x not in stopindewords]\n",
    "    return abstract_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next function is `get_stat()` which calculate the frequency of word on the list and grab the most 10 frequent word.\n",
    "\n",
    "Note that the list of the most frequent word is based on alphaetical ascending order in the case that the word appears as many time as the other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_stat(value):\n",
    "    stat = FreqDist(value).most_common(10)\n",
    "    stat = sorted(stat, key = lambda x: (-x[1], x[0]))\n",
    "    stat = [x[0] for x in stat]\n",
    "    return stat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are ready to perform the statistic generation!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create list for title, author and abstract respectively\n",
    "title_list = []\n",
    "author_list = []\n",
    "abstract_list = []\n",
    "#regex to match the pattern for title, author and abstract\n",
    "reg3 = re.compile(r'Authored\\sby:([\\s\\S]*?)Abstract')\n",
    "reg4 = re.compile(r'([\\s\\S]+?)Authored\\sby')\n",
    "reg5 = re.compile(r'Abstract([\\s\\S]+?)1\\sPaper\\sBody')\n",
    "#insert matched pattern into those lists \n",
    "for key, value in files.items():\n",
    "    if reg4.search(value):\n",
    "        title_list.append(reg4.search(value).group(1))\n",
    "    if reg3.search(value):\n",
    "        author_list.append(reg3.search(value).group(1))\n",
    "    if reg5.search(value):\n",
    "        abstract_list.append(reg5.search(value).group(1))\n",
    "#combine all the three list\n",
    "combine_list = list(zip(get_stat(get_title(title_list)), \\\n",
    "                        get_stat(get_abstract(abstract_list)), get_stat(get_author(author_list)))) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see from above, we wrap the function get_title(), get_abstract() and get_author() for the list of words and finally use get_stat() to complete the task.\n",
    "The result of combined list is shown below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('learning', 'data', 'Bernhard Sch?lkopf'),\n",
       " ('neural', 'learning', 'Lawrence Carin'),\n",
       " ('deep', 'model', 'Peter Dayan'),\n",
       " ('inference', 'problem', 'Piyush Rai'),\n",
       " ('gaussian', 'show', 'Quoc V. Le'),\n",
       " ('optimization', 'algorithm', 'Shivani Agarwal'),\n",
       " ('sparse', 'method', 'Zohar S. Karnin'),\n",
       " ('markov', 'models', 'Eric P. Xing'),\n",
       " ('analysis', 'approach', 'J?rgen Schmidhuber'),\n",
       " ('model', 'methods', 'Jun Zhu')]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combine_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that you may see the `?` in some of the name list as a result of pdf file that also have the `?` appear on some of the names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#put the list in datafram and give the column name\n",
    "df = pd.DataFrame(combine_list, columns= ['top10_terms_in_abstracts','top10_terms_in_titles','top10_authors'])\n",
    "#export dataframe to csv file\n",
    "df.to_csv('Group148_stats.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Summary\n",
    "This assessment measured the understanding of textual data analyzing techniques in the Python programming language. The main outcomes achieved while applying these techniques were:\n",
    "\n",
    "- **Data extraction**. By using the built-in package called `re` or Regex module. With functions like `search()` and `sub()`, allow us to spot all the pattern of data with only a few inspections inluding replace unecessary characters in a proper way.\n",
    "- **Pre-Processed text and Generating Features**. By using the `nltk` package, converting unstructured and semi-structured text into a structured representation before text analysis, then generate different vector representations and learn how to compute some basic statistics for text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7.Reference\n",
    "\n",
    "- garbage, D. (2019). Downloading a pdf file in python with urllib results in some html garbage. Retrieved 14 September 2019, from https://stackoverflow.com/questions/52273055/downloading-a-pdf-file-in-python-with-urllib-results-in-some-html-garbage\n",
    "- PDF, R., Pietzcker, T., & Thoma, M. (2019). Read PDF in Python and convert to text in PDF. Retrieved 14 September 2019, from https://stackoverflow.com/questions/23821204/read-pdf-in-python-and-convert-to-text-in-pdf\n",
    "- Dib, F. (2019). Online regex tester and debugger: PHP, PCRE, Python, Golang and JavaScript. Retrieved 22 August 2019, from https://regex101.com\n",
    "- Exploring Pre-Processed text and Generating Features. (2019). .ipynb. (from moodle week5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
